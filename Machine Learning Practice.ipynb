{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Process:-\n",
    "\n",
    "Credit to: `Jason Brownlee`\n",
    "\n",
    "Instructor: `Ainembabazi Moses`\n",
    "\n",
    "Prerequisite: Numpy, Pandas, Matplotlib\n",
    "\n",
    "Duration: 3Hrs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions of Your Data\n",
    "You must have a very good handle on how much data you have, both in terms of rows and columns.\n",
    "\n",
    "- `Data Integrity:` The first and foremost reason for checking the dimensions of data is to ensure that the data is complete and accurate. The shape of the data (number of rows and columns) can give an idea of how much data is available for analysis, and can help detect any inconsistencies or missing values in the data.\n",
    "\n",
    "- `Data Preprocessing:` The dimensions of data are important to determine the appropriate preprocessing steps to apply before building a machine learning model. For example, if there are too many missing values or too few data points, certain data cleaning or feature engineering techniques may need to be applied to prepare the data for analysis.\n",
    "\n",
    "- `Model Training:` The dimensions of data also impact the training of machine learning models. Machine learning models require a certain amount of data to learn and generalize patterns in the data. If the dataset is too small, the model may not learn the underlying patterns in the data, and if the dataset is too large, it may lead to overfitting. Thus, understanding the dimensions of the data is crucial for selecting an appropriate model and tuning its hyperparameters.\n",
    "\n",
    "\n",
    "- `Training time estimation:`  the dimensions of data can have a significant impact on the training times of machine learning models. In general, as the size of the dataset (i.e., number of rows and columns) increases, the training time of machine learning models also increases.\n",
    "This is because most machine learning algorithms require multiple passes over the data to learn the underlying patterns and relationships in the data. As the dataset size increases, the number of calculations required to process the data increases, which can result in longer training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "It is important to know your data types for a couple of reasons:-\n",
    "\n",
    "- `Data consistency:` Checking the data types ensures that the data is consistent and in the correct format. Inconsistent data types can lead to errors or incorrect predictions during model training and testing. For example, when you keep the dates they should be in one format such as \"mm/dd/yyyy\" if they shift around you will have errors in analysis\n",
    "\n",
    "- `Feature engineering:` Data types can determine how features are engineered for the model. For example, categorical data requires different feature engineering methods than numerical data. You will be able to plan properly\n",
    "\n",
    "- `Memory optimization:` Checking data types can help optimize memory usage during preprocessing, especially when dealing with large datasets. Data types can be converted to lower memory-consuming types like integers or floats.\n",
    "\n",
    "- `Efficient data manipulation`: Different data types can require different manipulation techniques, and knowing the data types beforehand can save time and improve efficiency during data manipulation.\n",
    "\n",
    "- `Model performance:` Checking the data types can help to identify potential issues that can affect model performance, such as non-numeric data types that require encoding or missing values that require imputation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "There are several important reasons why you would want to check the descriptive statistics of your data in a machine learning experiment. Here are some of the key reasons:\n",
    "\n",
    "- `Identifying missing or erroneous data:` Descriptive statistics can help you identify if any of your data is missing or if there are any errors in your dataset. For example, you may notice that certain features have a lower count of non-null values than others, which could indicate missing data.\n",
    "\n",
    "- `Understanding the distribution of your data:` Descriptive statistics can give you insights into the distribution of your data, such as the mean, median, and mode. This information can be useful for choosing the appropriate machine learning algorithms, as some algorithms work better with certain types of data distributions.\n",
    "\n",
    "- `Detecting outliers:` Descriptive statistics can also help you detect outliers, which are data points that fall outside the expected range of values. Outliers can have a significant impact on your machine learning model's performance, so it's important to identify and handle them appropriately.\n",
    "\n",
    "- `Assessing the quality of your data:` By examining the descriptive statistics of your data, you can get a sense of the overall quality of your dataset. For example, if you notice that some features have a large range of values compared to others, this could indicate that the data is noisy or that some features may be more important than others.\n",
    "\n",
    "- Descriptive statistics `can give you great insight into the shape of each attribute`. Often you can create more summaries than you have time to review. The describe() function on the Pandas\n",
    "\n",
    "DataFrame lists 8 statistical properties of each attribute. They are:\n",
    "\n",
    "- Count.\n",
    "- Mean.\n",
    "- Standard Deviation.\n",
    "- Minimum Value.\n",
    "- 25th Percentile.\n",
    "- 50th Percentile (Median).\n",
    "- 75th Percentile.\n",
    "- Maximum Value.\n",
    "\n",
    "\n",
    "## Pandas Profiling\n",
    "\n",
    "- This can be made easier using a library called pandasprofiling, this library helps you draw all the graphs but you will still have to make decisions on those graphs to move forward.\n",
    "\n",
    "\n",
    "# Homework:\n",
    ">> Does it make sense to have BloodPressure of 0 in a person as you see in the data.describe below? How would you handle it?\n",
    "What methods would you use?\n",
    "\n",
    "- Install pandasprofiling\n",
    "- Use it to create an EDA (Exploratory Data Analysis Report)\n",
    "- Find a way to replace the data that is 0.00 (and doesnt make sense : mean : numpy, find all 0.0 values and replace them)\n",
    "- Clean your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Does it make sense to have BloodPressure of 0 in a person as you see in the data.describe below? How would you handle it?\n",
    "What methods would you use?\n",
    "\n",
    ">>>> It is highly unlikely and biologically impossible for a person to have a blood pressure of 0. Therefore, having a BloodPressure value of 0 in a given dataset is most likely a result of missing or incorrect data.\n",
    "\n",
    ">>>> To handle this issue, we can replace the 0 values with an appropriate value, such as the mean or median of the non-zero values. This approach assumes that the missing or incorrect values are randomly distributed and not biased towards any specific group or subgroup in the dataset.\n",
    ">>>> After replacing the 0 values with an appropriate value, it is important to perform data validation and sanity checks to ensure the data is reasonable and consistent with domain knowledge. Additionally, it is recommended to perform exploratory data analysis and visualization to identify any other potential issues with the data.\n",
    "\n",
    ">>>> To handle the issue of BloodPressure values being 0 in a given dataset, we can use the following methods:\n",
    "- Identify the rows where the BloodPressure value is 0 using boolean indexing in pandas.\n",
    "- Replace the 0 values with an appropriate value, such as the mean or median of non-zero BloodPressure values.\n",
    "- Perform data validation and sanity checks to ensure the data is reasonable and consistent with domain knowledge.\n",
    "- Perform exploratory data analysis and visualization to identify any other potential issues with the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install pandasprofiling\n",
    "- Use it to create an EDA (Exploratory Data Analysis Report)\n",
    "- Find a way to replace the data that is 0.00 (and doesnt make sense : mean : numpy, find all 0.0 values and replace them)\n",
    "- Clean your data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Distribution (Classiffication Only)\n",
    "\n",
    "Why would someone check the label distribution in an ML/AI experiment?\n",
    "\n",
    "- `Class imbalance:` In some datasets, one class may be significantly more prevalent than the others. This can lead to a class imbalance problem, where the model is biased towards the majority class and performs poorly on the minority class. Checking the label distribution can help identify if there is a class imbalance problem in the data.\n",
    "\n",
    "- `Model performance:` The label distribution can impact the model's performance. If the model is trained on a dataset with a skewed label distribution, it may not generalize well to unseen data. Checking the label distribution can help ensure that the model is trained on a representative sample of the data.\n",
    "\n",
    "- `Data preprocessing:` Depending on the label distribution, different preprocessing techniques may be needed. For example, in a dataset with imbalanced labels, oversampling or undersampling techniques may be needed to balance the classes. Checking the label distribution can help determine what preprocessing techniques are needed.\n",
    "\n",
    "- `Data collection:` The label distribution can provide insights into the data collection process. For example, if a certain label is overrepresented, it may indicate that the data was collected in a biased way. Checking the label distribution can help identify potential biases in the data collection process.\n",
    "\n",
    "- `Domain knowledge:` Checking the label distribution can help provide insights into the problem domain. For example, if a certain label is underrepresented, it may indicate that the problem is rare or difficult to identify. Checking the label distribution can help the modeler understand the problem domain better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "Correlation refers to the relationship between two variables and how they may or may not change together. The most common method for calculating correlation is Pearson's Correlation Coefficient, that assumes a normal distribution of the attributes involved. \n",
    "\n",
    "`A correlation of -1 or 1 shows a full negative or positive correlation respectively.` \n",
    "\n",
    "Whereas a value of 0 shows no correlation at all. Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in your dataset. `It is important to know the assumptions of the model you are using before using it`\n",
    "\n",
    "---\n",
    "\n",
    "### Model Assumptions:\n",
    "\n",
    "The supporting reasons for checking the assumptions of a model in an ML experiment, and how it affects how you handle data in the case of a linear regression model.\n",
    "\n",
    "- `Ensures validity of results:` Checking the assumptions of a linear regression model ensures the validity of the results. If the assumptions are not met, the results obtained from the model may not be reliable or valid. For example, if the assumption of linearity is not met, the results obtained from the model may not accurately reflect the relationship between the dependent and independent variables.\n",
    "\n",
    "- `Helps in selecting appropriate variables:` Checking the assumptions of a linear regression model helps in selecting appropriate variables for the model. For example, if the assumption of normality is not met, it may indicate that some variables may not be appropriate for the model or may need to be transformed.\n",
    "\n",
    "- `Improves model accuracy:` Checking the assumptions of a linear regression model can help in improving the accuracy of the model. For example, if the assumption of homoscedasticity is not met, it may indicate that the model needs to be transformed or that a different model may be more appropriate.\n",
    "\n",
    "- `Guides data preparation:` Checking the assumptions of a linear regression model guides data preparation. For example, if the assumption of independence is not met, it may indicate that the data needs to be collected differently or that some observations need to be excluded from the analysis.\n",
    "\n",
    "- `Provides insights into underlying data:` Checking the assumptions of a linear regression model provides insights into the underlying data. For example, if the assumption of normality is not met, it may indicate that there are outliers or that the data is skewed.\n",
    "\n",
    "#### `Comparison of Assumptions between Linear Regression and Logistic Regression`\n",
    "---\n",
    "\n",
    "`Assumptions of Linear Regression Model:`\n",
    "\n",
    "- Linearity: The relationship between the dependent variable and independent variable is linear.\n",
    "- Independence: The observations are independent of each other.\n",
    "- Homoscedasticity: The variance of the errors is constant across all levels of the independent variable.\n",
    "- Normality: The errors follow a normal distribution.\n",
    "- No multicollinearity: There is no high correlation among the independent variables.\n",
    "\n",
    "`Assumptions of Logistic Regression Model:`\n",
    "\n",
    "- Linearity of the logit: The relationship between the independent variable and the logit of the dependent variable is linear.\n",
    "- Independence: The observations are independent of each other.\n",
    "- No multicollinearity: There is no high correlation among the independent variables.\n",
    "- Large sample size: There are a sufficient number of observations for each combination of the independent variables.\n",
    "- No outliers: There are no extreme observations that can influence the results.\n",
    "\n",
    "---\n",
    "\n",
    "As such, it is a good idea to review all of the pairwise correlations of the attributes in your dataset. You can use the corr() function on the Pandas DataFrame to calculate a correlation matrix.\n",
    "\n",
    "There are several reasons why one would consider the correlation between variables while doing machine learning:\n",
    "\n",
    "- `Identify redundant features:` Correlated features can be redundant and provide similar information. Including highly correlated features in a model can lead to overfitting and decrease the model's performance. Identifying and removing these features can help reduce the complexity of the model and improve its accuracy.\n",
    "\n",
    ">> Suppose you are building a model to predict the price of a house based on its size, number of bedrooms, number of bathrooms, and location. The number of bedrooms and number of bathrooms are highly correlated, and including both features in the model can lead to overfitting. By identifying the correlation between these two features and removing one of them, we can reduce the complexity of the model.\n",
    "\n",
    "- `Improve model interpretability:` Understanding the correlation between variables can help to explain the relationships between the features and the target variable. This understanding can improve the interpretability of the model and help to identify important features that contribute most to the prediction.\n",
    "\n",
    ">> Consider a model that predicts whether a customer will buy a product based on their age, income, and education level. By analyzing the correlation between these features and the target variable, we can determine that income has the strongest correlation with the target variable. This understanding can help to explain why income is a critical factor in determining whether a customer buys a product, and improve the interpretability of the model.\n",
    "\n",
    "- `Data preprocessing:` Correlation analysis can be used to identify and remove outliers and missing values in the dataset. This process can help to improve the quality of the data and the model's performance.\n",
    "\n",
    ">> Suppose you are building a model to predict the likelihood of a customer to churn (i.e., leave) a subscription-based service. The dataset includes missing values and outliers, which can affect the quality of the data and the model's performance. By analyzing the correlation between the features and the target variable, we can identify and remove outliers and fill in missing values, which can help to improve the quality of the data and the model's performance.\n",
    "\n",
    "- `Feature engineering:` Correlation analysis can help in creating new features that can improve the model's performance. For example, we can create a new feature by combining highly correlated features or by performing operations on correlated features to create a new feature that is more predictive.\n",
    "\n",
    ">> Consider a model that predicts whether a student will pass a test based on their study hours, attendance rate, and average grade. By analyzing the correlation between these features, we can create a new feature by multiplying the study hours and attendance rate, which are highly correlated. This new feature can be more predictive than either feature alone, and can help to improve the model's performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Feature and Label Correlation\n",
    "`Feature selection:` Correlation analysis can help identify which variables are strongly correlated with the label and, therefore, are potentially good features for use in the model. Feature 
selection is important to prevent overfitting and improve the accuracy of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Your Data For Machine Learning\n",
    "\n",
    "Many machine learning algorithms make assumptions about your data. It is often a very good\n",
    "idea to prepare your data in such way to best expose the structure of the problem to the machine\n",
    "learning algorithms that you intend to use. In this chapter you will discover how to prepare\n",
    "your data for machine learning in Python using scikit-learn. After completing this lesson you\n",
    "will know how to:\n",
    "\n",
    "1. Rescale data.\n",
    "2. Standardize data.\n",
    "3. Normalize data.\n",
    "4. Binarize data.\n",
    "\n",
    "\n",
    "### Need For Data Pre-processing\n",
    "You almost always need to pre-process your data. It is a required step. A difficulty is that\n",
    "different algorithms make different assumptions about your data and may require different\n",
    "transforms. Further, when you follow all of the rules and prepare your data, sometimes algorithms\n",
    "can deliver better results without pre-processing.\n",
    "Generally, I would recommend creating many different views and transforms of your data,\n",
    "then exercise a handful of algorithms on each view of your dataset. This will help you to \n",
    "ush\n",
    "out which data transforms might be better at exposing the structure of your problem in general.\n",
    "\n",
    "The steps involved are as below:\n",
    "\n",
    "- Split the dataset into the input and output variables for machine learning.\n",
    "- Apply a pre-processing transform to the input variables.\n",
    "- Summarize the data to show the change.\n",
    "\n",
    "The scikit-learn library provides two standard idioms for transforming data. Each are useful\n",
    "in different circumstances. The transforms are calculated in such a way that they can be applied\n",
    "to your training data and any samples of data you may have in the future. The scikit-learn\n",
    "documentation has some information on how to use various different pre-processing methods:\n",
    "\n",
    "The Fit and Multiple Transform method is the preferred approach. You call the fit()\n",
    "function to prepare the parameters of the transform once on your data. Then later you can use\n",
    "the transform() function on the same data to prepare it for modeling and again on the test or\n",
    "validation dataset or new data that you may see in the future. The Combined Fit-And-Transform\n",
    "is a convenience that you can use for one of the tasks. This might be useful if you are interested\n",
    "in plotting or summarizing the transformed data.\n",
    "\n",
    "\n",
    "## Rescale Data\n",
    "\n",
    "In machine learning, `rescaling refers to the process of transforming the values of a variable to a new scale`, typically to a specific range or distribution. Rescaling is often used as a data preprocessing step to prepare the data for use in a machine learning model.\n",
    "\n",
    "Rescaling can be achieved using various techniques, including normalization and standardization, as well as other methods such as `min-max scaling` and `log transformation`. The choice of rescaling technique depends on the nature of the data and the specific requirements of the model.\n",
    "\n",
    "Rescaling is important in machine learning because it can help to improve the performance of the model by reducing the impact of features that have large values or different scales. Rescaling can also help to simplify the interpretation of the model by making the features more comparable and understandable.\n",
    "\n",
    "- `Avoiding numerical instability:` In some machine learning algorithms, such as gradient-based optimization algorithms, the scale of the input features can affect the convergence of the algorithm. Rescaling the data can help avoid numerical instability and improve the convergence of the algorithm.\n",
    "\n",
    "- `Improving performance:` Rescaling the data can improve the performance of the model, especially when using distance-based algorithms or when the features have different scales. In distance-based algorithms, the distance between two data points is affected by the scale of the features, so rescaling the data can make the distance more meaningful. Similarly, when the features have different scales, rescaling can help give equal importance to all features.\n",
    "\n",
    "- `Reducing computational complexity:` Rescaling the data can reduce the computational complexity of some machine learning algorithms, such as support vector machines. When the features have different scales, some algorithms may require more computation to converge, and rescaling can help reduce this complexity.\n",
    "\n",
    "- `Normalization:` As mentioned in the previous answer, rescaling can be used for normalization, which is a technique used to rescale data to a specific range, typically between 0 and 1. Normalization can help in comparing variables that have different units or scales or when we want to give more importance to small values.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data\n",
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and\n",
    "differing means and standard deviations to a standard Gaussian distribution with a mean of\n",
    "0 and a standard deviation of 1. \n",
    "\n",
    "`Standardization`, also known as `z-score normalization`, is a technique used to `transform data so that it has zero mean and unit variance`. This is achieved by `subtracting the mean of the data and dividing by its standard deviation.` The resulting transformed data has `a mean of 0` and a `standard deviation of 1`. Standardization is often used when the distribution of the data is approximately normal or when we want to give equal importance to all features.\n",
    "\n",
    "__`It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis.`__ \n",
    "\n",
    "Example in Skearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data\n",
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called\n",
    "a unit norm or a vector with the length of 1 in linear algebra).\n",
    "\n",
    "`Normalization`, on the other hand, is `a technique used to rescale data to a specific range, typically between 0 and 1.` This is achieved by `subtracting the minimum value of the data and dividing by the range` (i.e., the difference between the maximum and minimum values). Normalization is often used when we want to compare variables that have different units or scales or when we want to give more importance to small values.\n",
    "\n",
    "__`This pre-processing method can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as neural networks and algorithms that use distance measures such as k-Nearest Neighbors.`__ \n",
    "\n",
    "Example in Skearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Spot-Checking\n",
    "\n",
    "Algorithm spot-checking is a process in machine learning where multiple algorithms are tested and compared on a specific dataset to identify the one that performs the best for a given task. It involves evaluating the performance of various algorithms using a set of standard evaluation metrics, such as accuracy, precision, recall, F1 score, and ROC-AUC score, among others. The goal of algorithm spot-checking is to find the best algorithm that can accurately model the underlying pattern in the data and generalize well to new, unseen data. By trying out multiple algorithms and comparing their performance, algorithm spot-checking helps data scientists and machine learning practitioners to identify the best algorithm for their specific task and improve the overall performance of their models.\n",
    "\n",
    "- `Determine the Best Algorithm:` Spot-checking allows you to compare the performance of multiple algorithms on the same dataset and select the best one for your specific task. Different algorithms are suited to different types of problems, and spot-checking helps you find the one that performs the best on your data.\n",
    "\n",
    "- `Save Time:` Trying out multiple algorithms manually can be time-consuming and inefficient. With algorithm spot-checking, you can automate the process of testing different models, which can save you time and effort.\n",
    "\n",
    "- `Improve Accuracy:` By testing multiple algorithms and comparing their performance, you can improve the accuracy of your model. This is because different algorithms have different strengths and weaknesses, and by choosing the best one for your task, you can improve the overall accuracy of your model.\n",
    "\n",
    "- `Reduce Overfitting:` Overfitting occurs when a model is too complex and captures noise instead of the underlying pattern in the data. By comparing the performance of multiple algorithms, you can identify the one that is less prone to overfitting and select it for your model.\n",
    "\n",
    "- `Gain Insights:` Algorithm spot-checking can also provide valuable insights into the characteristics of your data. By analyzing the performance of different algorithms, you can gain a better understanding of the patterns and relationships within your data, which can help you develop better models in the future.\n",
    "\n",
    "\n",
    "\n",
    "#### Algorithms Overview\n",
    "We are going to take a look at six classification algorithms that you can spot-check on your\n",
    "dataset. Starting with two linear machine learning algorithms:\n",
    "- Logistic Regression.\n",
    "- Linear Discriminant Analysis.\n",
    "- Naive Bayes.\n",
    "\n",
    "Then looking at four nonlinear machine learning algorithms:\n",
    "- k-Nearest Neighbors.\n",
    "- Classification and Regression Trees.\n",
    "- Support Vector Machines.\n",
    "\n",
    "### Linear Machine Learning Algorithms\n",
    "This section demonstrates minimal recipes for how to use two linear machine learning algorithms:\n",
    "`logistic regression` and `linear discriminant analysis`.\n",
    "\n",
    "#### Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Machine Learning Algorithms\n",
    "\n",
    "### k-Nearest Neighbors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classiffication and Regression Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot-Check Regression Algorithms\n",
    "\n",
    "- Linear Regression.\n",
    "- Ridge Regression.\n",
    "- LASSO Linear Regression.\n",
    "- Elastic Net Regression.\n",
    "\n",
    "Then looking at three nonlinear machine learning algorithms:\n",
    "- k-Nearest Neighbors.\n",
    "- Classification and Regression Trees.\n",
    "- Support Vector Machines.\n",
    "\n",
    "### Linear Regression\n",
    "Linear regression assumes that the input variables have a Gaussian distribution. It is also\n",
    "assumed that input variables are relevant to the output variable and that they are not highly\n",
    "correlated with each other (a problem called collinearity). You can construct a linear regression\n",
    "model using the LinearRegression class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Ridge regression is an extension of linear regression where the loss function is modified to\n",
    "minimize the complexity of the model measured as the sum squared value of the coefficient\n",
    "values (also called the L2-norm). You can construct a ridge regression model by using the Ridge\n",
    "class2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Machine Learning Algorithms\n",
    "This section provides examples of how to use three di\u000b\n",
    "\n",
    "erent nonlinear machine learning algorithms\n",
    "for regression in Python with scikit-learn.\n",
    "\n",
    "### K-Nearest Neighbors\n",
    "The k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the\n",
    "training dataset for a new data instance. From the k neighbors, a mean or median output\n",
    "variable is taken as the prediction. Of note is the distance metric used (the metric argument).\n",
    "The Minkowski distance is used by default, which is a generalization of both the Euclidean\n",
    "distance (used when all inputs have the same scale) and Manhattan distance (for when the\n",
    "scales of the input variables di\u000b\n",
    "\n",
    "er). You can construct a KNN model for regression using the\n",
    "KNeighborsRegressor class5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Machine Learning Algorithms\n",
    "It is important to compare the performance of multiple di\u000b\n",
    "\n",
    "erent machine learning algorithms\n",
    "consistently. In this chapter you will discover how you can create a test harness to compare\n",
    "multiple different machine learning algorithms in Python with scikit-learn. You can use this\n",
    "test harness as a template on your own machine learning problems and add more and different\n",
    "algorithms to compare. After completing this lesson you will know:\n",
    "\n",
    "1. How to formulate an experiment to directly compare machine learning algorithms.\n",
    "2. A reusable template for evaluating the performance of multiple algorithms on one dataset.\n",
    "3. How to report and visualize the results when comparing algorithm performance.\n",
    "\n",
    "\n",
    "### Choose The Best Machine Learning Model\n",
    "When you work on a machine learning project, you often end up with multiple good models\n",
    "to choose from. Each model will have different performance characteristics. Using resampling\n",
    "methods like cross validation, you can get an estimate for how accurate each model may be on\n",
    "unseen data. You need to be able to use these estimates to choose one or two best models from\n",
    "the suite of models that you have created.\n",
    "When you have a new dataset, it is a good idea to visualize the data using different techniques\n",
    "in order to look at the data from di\u000b\n",
    "\n",
    "erent perspectives. The same idea applies to model selection.\n",
    "You should use a number of di\u000b\n",
    "\n",
    "erent ways of looking at the estimated accuracy of your machine\n",
    "learning algorithms in order to choose the one or two algorithm to finalize. A way to do this is\n",
    "to use visualization methods to show the average accuracy, variance and other properties of the\n",
    "distribution of model accuracies. In the next section you will discover exactly how you can do\n",
    "that in Python with scikit-learn.\n",
    "\n",
    "\n",
    "### Compare Machine Learning Algorithms Consistently\n",
    "The key to a fair comparison of machine learning algorithms is ensuring that each algorithm is\n",
    "evaluated in the same way on the same data. You can achieve this by forcing each algorithm to be evaluated on a consistent test harness. In the example below six different classiffication\n",
    "algorithms are compared on a single dataset:\n",
    "\n",
    "- Logistic Regression.\n",
    "- Linear Discriminant Analysis.\n",
    "- k-Nearest Neighbors.\n",
    "- Classiffication and Regression Trees.\n",
    "- Naive Bayes.\n",
    "- Support Vector Machines.\n",
    "\n",
    "The dataset is the Pima Indians onset of diabetes problem. The problem has two classes and\n",
    "eight numeric input variables of varying scales. The 10-fold cross validation procedure is used to\n",
    "evaluate each algorithm, importantly con\f\n",
    "\n",
    "gured with the same random seed to ensure that the\n",
    "same splits to the training data are performed and that each algorithm is evaluated in precisely\n",
    "the same way. Each algorithm is given a short name, useful for summarizing results afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, it would suggest that both logistic regression and linear discriminant analysis are perhaps worthy of further study on this problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chosen Model: Logistic Regression\n",
    "\n",
    "- `Problem Representation:` Logistic regression is used for binary classification problems, where the goal is to predict one of two possible outcomes (e.g., spam or not spam, pass or fail). The output of logistic regression is a probability value between 0 and 1.\n",
    "\n",
    "- `Linear Function:` In logistic regression, we first create a linear function by multiplying input features (x) with their corresponding weights (w) and adding a bias term (b). This is similar to linear regression but with a key difference in the next step.\n",
    "\n",
    ">> Mathematically: z = w1 * x1 + w2 * x2 + ... + wn * xn + b\n",
    "\n",
    "- `Sigmoid Function:` The linear function's output (z) is passed through the sigmoid function (also called the logistic function) to convert it into a probability value between 0 and 1. The sigmoid function has an S-shaped curve, which maps any real-valued number to a value between 0 and 1.\n",
    "\n",
    ">> Sigmoid function: p = 1 / (1 + e^(-z))\n",
    "\n",
    "- `Model Training:` To train the logistic regression model, we need to find the optimal weights (w) and bias (b) that minimize the error between predicted probabilities and the actual outcomes. This is done using a technique called gradient descent, which iteratively adjusts the weights and bias to minimize the cost function (e.g., cross-entropy loss).\n",
    "\n",
    "- `Prediction and Decision Boundary:` Once the model is trained, it can be used to predict the probability of a given input belonging to a particular class. A decision boundary is set (typically at 0.5), and if the probability is greater than this threshold, the input is classified as one class; otherwise, it is classified as the other class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation (Model Evaluation)\n",
    "\n",
    "Cross-validation is a widely used technique in machine learning to assess the performance of a model and its ability to generalize to unseen data. It is important because it helps to address the issue of `overfitting`, where a model learns the training data too well and performs `poorly on new, unseen data`. \n",
    "\n",
    ">> `Cross-validation provides a more accurate estimate of a model's performance on unseen data and reduces the risk of overfitting.`\n",
    "\n",
    "The key idea behind cross-validation is to divide the dataset into multiple smaller subsets, called folds, and train and evaluate the model multiple times. Each time, the model is trained on a different combination of folds and tested on the remaining fold. This process is repeated until every fold has been used for testing exactly once. The performance of the model is then averaged across all iterations to provide a more reliable estimate of its performance.\n",
    "\n",
    "Here are the main reasons why cross-validation is important in machine learning:\n",
    "\n",
    "`More accurate performance estimation:` By evaluating the model on different subsets of data, cross-validation provides a better estimate of the model's performance on unseen data compared to a single train-test split.\n",
    "\n",
    "`Reducing overfitting:` Overfitting occurs when a model learns the training data too well, including the noise, and fails to generalize to new data. Cross-validation can help detect overfitting by providing performance estimates on different data subsets, thus encouraging the selection of a model with better generalization ability.\n",
    "\n",
    "`Model selection and hyperparameter tuning:` Cross-validation can be used to compare the performance of different models or to fine-tune the hyperparameters of a single model. By comparing cross-validation scores, you can select the model or hyperparameter configuration that performs best on average across different data subsets.\n",
    "\n",
    "`Efficient use of data:` In situations where the available dataset is limited, cross-validation allows for more efficient use of the data by training and testing the model on different subsets, ensuring that the model's performance is assessed on the entire dataset.\n",
    "\n",
    "One popular form of cross-validation is `k-fold cross-validation`, where the dataset is divided into k equal-sized folds. The model is then trained and tested k times, each time using a different fold for testing and the remaining k-1 folds for training. Other variants of cross-validation include stratified k-fold, which preserves the class distribution in each fold, and leave-one-out cross-validation, where each sample is used as a test set exactly once."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation (Cross Validation)\n",
    "\n",
    "Evaluating if a model is `overfitting` based on `cross-validation (CV)` scores and training scores involves comparing the model's performance on the training set and the cross-validation set. Here's a simple guideline to interpret these scores:\n",
    "\n",
    "- If the training score is high and the CV score is also high, the model is likely performing well and generalizing effectively to unseen data. This indicates a good balance between bias and variance.\n",
    "\n",
    "- If the training score is high, but the CV score is significantly lower, the model is likely overfitting. Overfitting occurs when the model captures the noise in the training data and is unable to generalize well to new, unseen data. In this case, you should consider simplifying the model (e.g., using regularization, reducing the number of features, or using a less complex model) or increasing the size of the training set.\n",
    "\n",
    "- If both the training score and CV score are low, the model is likely underfitting. This occurs when the model is too simple to capture the underlying patterns in the data. To address underfitting, you can try using a more complex model, adding more features, or fine-tuning the model's hyperparameters.\n",
    "\n",
    "`It's important to note that comparing the absolute values of the scores may not always be informative.`\n",
    " Instead, look at the `relative difference between the training and CV scores to assess if the model is overfitting.` A large gap between the two scores is indicative of overfitting, while a smaller gap suggests a better balance between bias and variance.\n",
    "\n",
    ">> Keep in mind that cross-validation provides a more robust evaluation of model performance than a single train-test split. By training and evaluating the model on different subsets of the data, cross-validation helps to reduce the risk of overfitting and provides a better estimate of the model's generalization ability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Individual Project Assignment:-\n",
    "\n",
    "You are a lead data scientist on a project regarding maternal health, you are provided with a dataset regarding featal health. (Please find the dataset in the data folder given). These are your instructions\n",
    "1 - Use that dataset to do: EDA, Clean the data\n",
    "2 - Use mandas Profiling to export a pdf report of your data (explain how the report informs your future decisions)\n",
    "3 - Apply any needed transformations if possible.\n",
    "4 - Choose a proper evaluation metric for your project.\n",
    "5 - Do model spot checking to find the best model for your use.\n",
    "6 - Build the chosen model\n",
    "7 - Evaluate bias vs variance on your model and present your results.\n",
    "\n",
    "Note:\n",
    "- Please follow instructions!\n",
    "- You have to explain in detail every step you are executing in your own words, your result will be based on how well you explain what steps you are executing.\n",
    "- Students are allowed to use public resources off the internet for their research, however students are not allowed to copy and paste code for this assignment (If you are caught, you will fail this assignment, you have been warned: especially chatGpt.)\n",
    "- This is a graded individual assignment any malpractice will lead to losing of points, this assignment contributes a great percentage to your final grade.\n",
    "- All assignments must be handed in by Close of Business next Wednesday by 6:00PM Date: 3rd - May - 2023\n",
    "- All your assignments will be handed in by means of a pull request to my original repository.\n",
    "- Please name your pull requests as per previously instructed, you will not get warnings for failing to do this.\n",
    "- Your work should be saved in the Project Folder (Create your own folder and give the folder your name.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate Machine Learning Workflows with Pipelines\n",
    "---\n",
    "\n",
    "<img src=\"https://www.wordstream.com/wp-content/uploads/2021/07/machine-learning.png\">\n",
    "\n",
    "---\n",
    "\n",
    "There are standard workflows in a machine learning project that can be automated. In Python\n",
    "scikit-learn, Pipelines help to clearly define and automate these work\n",
    "ows. In this chapter you will discover Pipelines in scikit-learn and how you can automate common machine learning\n",
    "work\n",
    "ows. After completing this lesson you will know:\n",
    "\n",
    "1. How to use pipelines to minimize data leakage.\n",
    "2. How to construct a data preparation and modeling pipeline.\n",
    "3. How to construct a feature extraction and modeling pipeline.\n",
    "\n",
    "### Automating Machine Learning Workflows\n",
    "There are standard work\n",
    "ows in applied machine learning. Standard because they overcome\n",
    "common problems like data leakage in your test harness. Python scikit-learn provides a Pipeline\n",
    "utility to help automate machine learning work\n",
    "ows. Pipelines work by allowing for a linear\n",
    "sequence of data transforms to be chained together culminating in a modeling process that can\n",
    "be evaluated.\n",
    "\n",
    "The goal is to ensure that all of the steps in the pipeline are constrained to the data available\n",
    "for the evaluation, such as the training dataset or each fold of the cross validation procedure.\n",
    "\n",
    "### Data Preparation and Modeling Pipeline\n",
    "An easy trap to fall into in applied machine learning is leaking data from your training dataset\n",
    "to your test dataset. To avoid this trap you need a robust test harness with strong separation of training and testing. This includes data preparation. Data preparation is one easy way to leak\n",
    "knowledge of the whole training dataset to the algorithm. For example, preparing your data\n",
    "using normalization or standardization on the entire training dataset before learning would not\n",
    "be a valid test because the training dataset would have been in\n",
    "uenced by the scale of the data\n",
    "in the test set.\n",
    "\n",
    "\n",
    "Pipelines help you prevent data leakage in your test harness by ensuring that data preparation\n",
    "like standardization is constrained to each fold of your cross validation procedure. The example\n",
    "below demonstrates this important data preparation and model evaluation work\n",
    "ow on the Pima Indians onset of diabetes dataset. The pipeline is defined with two steps:\n",
    "\n",
    "1. Standardize the data.\n",
    "2. Learn a Linear Discriminant Analysis model.\n",
    "\n",
    "The pipeline is then evaluated using 10-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we create a Python list of steps that are provided to the Pipeline for process\n",
    "the data. Also notice how the Pipeline itself is treated like an estimator and is evaluated in its\n",
    "entirety by the k-fold cross validation procedure. Running the example provides a summary of\n",
    "accuracy of the setup on the dataset.\n",
    "\n",
    "\n",
    "## Feature Extraction and Modeling Pipeline\n",
    "Feature extraction is another procedure that is susceptible to data leakage. Like data preparation,\n",
    "feature extraction procedures must be restricted to the data in your training dataset. The\n",
    "pipeline provides a handy tool called the FeatureUnion which allows the results of multiple\n",
    "feature selection and extraction procedures to be combined into a larger dataset on which a\n",
    "model can be trained. Importantly, all the feature extraction and the feature union occurs\n",
    "within each fold of the cross validation procedure. The example below demonstrates the pipeline\n",
    "defined with four steps:\n",
    "\n",
    "1. Feature Extraction with Principal Component Analysis (3 features).\n",
    "2. Feature Extraction with Statistical Selection (6 features).\n",
    "3. Feature Union.\n",
    "4. Learn a Logistic Regression Model.\n",
    "\n",
    "The pipeline is then evaluated using 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Performance with Ensembles\n",
    "\n",
    "Ensembles can give you a boost in accuracy on your dataset. In this chapter you will discover\n",
    "how you can create some of the most powerful types of ensembles in Python using scikit-learn.\n",
    "This lesson will step you through Boosting, Bagging and Majority Voting and show you how you\n",
    "can continue to ratchet up the accuracy of the models on your own datasets. After completing\n",
    "this lesson you will know:\n",
    "\n",
    "1. How to use bagging ensemble methods such as bagged decision trees, random forest and extra trees.\n",
    "2. How to use boosting ensemble methods such as AdaBoost and stochastic gradient boosting.\n",
    "3. How to use voting ensemble methods to combine the predictions from multiple algorithms.\n",
    "\n",
    "### Combine Models Into Ensemble Predictions\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "- Bagging. Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "- Boosting. Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.\n",
    "- Voting. Building multiple models (typically of di\u000b\n",
    "\n",
    "ering types) and simple statistics (like calculating the mean) are used to combine predictions.\n",
    "\n",
    "This assumes you are generally familiar with machine learning algorithms and ensemble\n",
    "methods and will not go into the details of how the algorithms work or their parameters.\n",
    "The Pima Indians onset of Diabetes dataset is used to demonstrate each algorithm. Each\n",
    "ensemble algorithm is demonstrated using 10-fold cross validation and the classiffication accuracy\n",
    "performance metric.\n",
    "\n",
    "\n",
    "### Bagging Algorithms\n",
    "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset\n",
    "(with replacement) and training a model for each sample. The \f\n",
    "\n",
    "nal output prediction is averaged\n",
    "across the predictions of all of the sub-models. The three bagging models covered in this section\n",
    "are as follows:\n",
    "\n",
    "- Bagged Decision Trees.\n",
    "- Random Forest.\n",
    "- Extra Trees.\n",
    "\n",
    "### Bagged Decision Trees\n",
    "Bagging performs best with algorithms that have high variance. A popular example are\n",
    "decision trees, often constructed without pruning. In the example below is an example\n",
    "of using the BaggingClassifier with the Classi\f\n",
    "\n",
    "cation and Regression Trees algorithm\n",
    "(DecisionTreeClassifier1). A total of 100 trees are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random Forests is an extension of bagged decision trees. Samples of the training dataset are\n",
    "taken with replacement, but the trees are constructed in a way that reduces the correlation\n",
    "between individual classiffiers. Specifically, rather than greedily choosing the best split point in\n",
    "the construction of each tree, only a random subset of features are considered for each split. You\n",
    "can construct a Random Forest model for classiffication using the RandomForestClassifier\n",
    "class2. The example below demonstrates using Random Forest for classiffication with 100 trees\n",
    "and split points chosen from a random selection of 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees\n",
    "Extra Trees are another modi\f\n",
    "\n",
    "cation of bagging where random trees are constructed from\n",
    "samples of the training dataset. You can construct an Extra Trees model for classiffication using\n",
    "the ExtraTreesClassifier class3. The example below provides a demonstration of extra trees\n",
    "with the number of trees set to 100 and splits chosen from 7 random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Algorithms\n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes\n",
    "of the models before them in the sequence. Once created, the models make predictions which\n",
    "may be weighted by their demonstrated accuracy and the results are combined to create a \f\n",
    "\n",
    "nal\n",
    "output prediction. The two most common boosting ensemble machine learning algorithms are:\n",
    "\n",
    "- AdaBoost.\n",
    "- Stochastic Gradient Boosting.\n",
    "\n",
    "\n",
    "### AdaBoost\n",
    "AdaBoost was perhaps the \f\n",
    "\n",
    "rst successful boosting ensemble algorithm. It generally works\n",
    "by weighting instances in the dataset by how easy or di\u000ecult they are to classify, allowing\n",
    "the algorithm to pay or less attention to them in the construction of subsequent models. You\n",
    "can construct an AdaBoost model for classi\f\n",
    "\n",
    "cation using the AdaBoostClassifier class4. The\n",
    "example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting\n",
    "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most\n",
    "sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of\n",
    "the best techniques available for improving performance via ensembles. You can construct a\n",
    "Gradient Boosting model for classiffication using the GradientBoostingClassifier class5. The\n",
    "example below demonstrates Stochastic Gradient Boosting for classi\f\n",
    "\n",
    "cation with 100 trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensemble\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning\n",
    "algorithms. It works by first creating two or more standalone models from your training dataset.\n",
    "A Voting Classiffier can then be used to wrap your models and average the predictions of the\n",
    "sub-models when asked to make predictions for new data. The predictions of the sub-models can\n",
    "be weighted, but specifying the weights for classiffiers manually or even heuristically is difficult.\n",
    "More advanced methods can learn how to best weight the predictions from sub-models, but this\n",
    "is called stacking (stacked aggregation) and is currently not provided in scikit-learn.\n",
    "You can create a voting ensemble model for classiffication using the VotingClassifier\n",
    "class6. The code below provides an example of combining the predictions of logistic regression,\n",
    "classiffication and regression trees and support vector machines together for a classiffication\n",
    "problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Your Model with pickle\n",
    "Pickle is the standard way of serializing objects in Python. You can use the pickle1 operation\n",
    "to serialize your machine learning algorithms and save the serialized format to a file. Later you\n",
    "can load this file to deserialize your model and use it to make new predictions. The example\n",
    "below demonstrates how you can train a logistic regression model on the Pima Indians onset of\n",
    "diabetes dataset, save the model to file and load it to make predictions on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Your Model with Joblib\n",
    "The Joblib2 library is part of the SciPy ecosystem and provides utilities for pipelining Python\n",
    "jobs. It provides utilities for saving and loading Python objects that make use of NumPy data\n",
    "structures, efficiently3. This can be useful for some machine learning algorithms that require a\n",
    "lot of parameters or store the entire dataset (e.g. k-Nearest Neighbors). The example below\n",
    "demonstrates how you can train a logistic regression model on the Pima Indians onset of diabetes\n",
    "dataset, save the model to file using Joblib and load it to make predictions on the unseen test\n",
    "set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Summary\n",
    "Below is the project template that you can use in your machine learning projects in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Project Template\n",
    "# 1. Prepare Problem\n",
    "# a) Load libraries\n",
    "# b) Load dataset\n",
    "\n",
    "# 2. Summarize Data\n",
    "# a) Descriptive statistics\n",
    "# b) Data visualizations\n",
    "\n",
    "# 3. Prepare Data\n",
    "# a) Data Cleaning\n",
    "# b) Feature Selection\n",
    "# c) Data Transforms\n",
    "\n",
    "# 4. Evaluate Algorithms\n",
    "# a) Split-out validation dataset\n",
    "# b) Test options and evaluation metric\n",
    "# c) Spot Check Algorithms\n",
    "# d) Compare Algorithms\n",
    "\n",
    "# 5. Improve Accuracy\n",
    "# a) Algorithm Tuning\n",
    "# b) Ensembles\n",
    "\n",
    "# 6. Finalize Model\n",
    "# a) Predictions on validation dataset\n",
    "# b) Create standalone model on entire training dataset\n",
    "# c) Save model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
